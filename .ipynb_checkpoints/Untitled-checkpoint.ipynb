{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import statsmodels as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_td = pd.read_csv('tic_2000_train_data.csv')\n",
    "eval1 = pd.read_csv('tic_2000_eval_data.csv')\n",
    "target = pd.read_csv('tic_2000_target_data.csv') #CARAVAN is renamed 'Target' in this set\n",
    "\n",
    "#renaming the training data to match the test data.\n",
    "df_td.rename(columns={'MOSTYPE': 'subtype_L0', 'MAANTHUI':'Num_houses', 'MGEMOMV' : 'Avg_hh_size',\n",
    "                   'MGEMLEEF':'age_L1', 'MOSHOOFD': 'maintype_L2', 'MGODRK': 'romcath_L3',\n",
    "                   'MGODPR': 'Protestant','MGODOV' : 'O_religion', 'MGODGE': 'N_religion','MRELGE' :'Married',\n",
    "                   'MRELSA' : 'Living_together','MRELOV' : 'O_relation','MFALLEEN' : 'Singles','MFGEKIND' : 'hh_wo_child',\n",
    "                   'MFWEKIND' : 'hh_w_child','MOPLHOOG' : 'H_lvl_edu','MOPLMIDD' : 'M_lvl_edu',\n",
    "                   'MOPLLAAG' : 'L_lvl_edu','MBERHOOG' : 'H_status','MBERZELF' : 'Entrepreneur','MBERBOER' : 'Farmer',\n",
    "                   'MBERMIDD' : 'Mid_management','MBERARBG' : 'Skld_labor','MBERARBO' : 'Unskld_labor',\n",
    "                   'MSKA' : 'Soc_cls_A','MSKB1' : 'Soc_cls_B1','MSKB2' : 'Soc_cls_B2','MSKC' : 'Soc_cls_C',\n",
    "                   'MSKD' : 'Soc_cls_D','MHHUUR' : 'R_house','MHKOOP' : 'O_house','MAUT1' : '1_car','MAUT2' : '2_cars',\n",
    "                   'MAUT0' : 'N_car','MZFONDS' : 'Nat_Hlth_Serv','MZPART' : 'Prv_Hlth_Insur','MINKM30' : 'Inc_u_30k',\n",
    "                   'MINK3045' : 'Inc_btw_30_45k','MINK4575' : 'Inc_btw_45_75k','MINK7512' : 'Inc_75_122k','MINK123M' : 'Inc_ovr_123k',\n",
    "                   'MINKGEM' : 'Avg_inc','MKOOPKLA' : 'PP_cls','PWAPART' : 'Contri_prv_3p_insur','PWABEDR' : 'Firm_Contri_3p_ insur',\n",
    "                   'PWALAND' : 'Ag_Contri_3p_insur','PPERSAUT' : 'Contri_car_pol','PBESAUT' : 'Contri_deliv_van_pol',\n",
    "                   'PMOTSCO' : 'Contri_motorcycle/scooter_pol','PVRAAUT' : 'Contri_lorry_pol','PAANHANG' : 'Contri_trailer_pols',\n",
    "                   'PTRACTOR' : 'Contri_tractor_pol','PWERKT' : 'Contri_ag_machine_pol','PBROM' : 'Contri_moped_pol',\n",
    "                   'PLEVEN' : 'Contri_life_insur','PPERSONG' : 'Contri_prv_accid_insur_pol',\n",
    "                   'PGEZONG' : 'Contri_fam_accid_insur_pol','PWAOREG' : 'Contri_disabl_insur_pol','PBRAND' : 'Contri_fire_pol',\n",
    "                   'PZEILPL' : 'Contri_surfb_pol','PPLEZIER' : 'Contri_boat_pol','PFIETS' : 'Contri_bike_pol',\n",
    "                   'PINBOED' : 'Contri_prop_insur_pol','PBYSTAND' : 'Contri_ss_insur_polo','AWAPART' : 'Num_prv_3p_insur',\n",
    "                   'AWABEDR' : 'Num_firm_3p_insur','AWALAND' : 'Num_ag_3p_insur','APERSAUT' : 'Num_car_pol',\n",
    "                   'ABESAUT' : 'Num_deliv_van_pol','AMOTSCO' : 'Num_motorcycle/scooter_pol', 'AVRAAUT' : 'Num_lorry_pol','AAANHANG': 'Num_trailer_pol',\n",
    "                   'ATRACTOR' : 'Num_tractor_pol','AWERKT' : 'Num_ag_machines_pol','ABROM' : 'Num_moped_pol',\n",
    "                   'ALEVEN' : 'Num_life_insur_pol', 'APERSONG' : 'Num_prv_accid_insur_pol','AGEZONG' : 'Num_fam_ccid_insur_pol',\n",
    "                   'AWAOREG' : 'Num_disabl_insur_pol','ABRAND' :'Num_fire_pol','AZEILPL' :'Num_surfb_pol','APLEZIER' :'Num_boat_pol',\n",
    "                   'AFIETS' :'Num_bike_pol','AINBOED' :'Num_prop_insur_pol','ABYSTAND' :'num_ss_insur_pol', 'CARAVAN' : 'Target'},\n",
    "          inplace=True)\n",
    "\n",
    "eval1.rename(columns={'MOSTYPE': 'subtype_L0', 'MAANTHUI':'Num_houses', 'MGEMOMV' : 'Avg_hh_size',\n",
    "                   'MGEMLEEF':'age_L1', 'MOSHOOFD': 'maintype_L2', 'MGODRK': 'romcath_L3',\n",
    "                   'MGODPR': 'Protestant','MGODOV' : 'O_religion', 'MGODGE': 'N_religion','MRELGE' :'Married',\n",
    "                   'MRELSA' : 'Living_together','MRELOV' : 'O_relation','MFALLEEN' : 'Singles','MFGEKIND' : 'hh_wo_child',\n",
    "                   'MFWEKIND' : 'hh_w_child','MOPLHOOG' : 'H_lvl_edu','MOPLMIDD' : 'M_lvl_edu',\n",
    "                   'MOPLLAAG' : 'L_lvl_edu','MBERHOOG' : 'H_status','MBERZELF' : 'Entrepreneur','MBERBOER' : 'Farmer',\n",
    "                   'MBERMIDD' : 'Mid_management','MBERARBG' : 'Skld_labor','MBERARBO' : 'Unskld_labor',\n",
    "                   'MSKA' : 'Soc_cls_A','MSKB1' : 'Soc_cls_B1','MSKB2' : 'Soc_cls_B2','MSKC' : 'Soc_cls_C',\n",
    "                   'MSKD' : 'Soc_cls_D','MHHUUR' : 'R_house','MHKOOP' : 'O_house','MAUT1' : '1_car','MAUT2' : '2_cars',\n",
    "                   'MAUT0' : 'N_car','MZFONDS' : 'Nat_Hlth_Serv','MZPART' : 'Prv_Hlth_Insur','MINKM30' : 'Inc_u_30k',\n",
    "                   'MINK3045' : 'Inc_btw_30_45k','MINK4575' : 'Inc_btw_45_75k','MINK7512' : 'Inc_75_122k','MINK123M' : 'Inc_ovr_123k',\n",
    "                   'MINKGEM' : 'Avg_inc','MKOOPKLA' : 'PP_cls','PWAPART' : 'Contri_prv_3p_insur','PWABEDR' : 'Firm_Contri_3p_ insur',\n",
    "                   'PWALAND' : 'Ag_Contri_3p_insur','PPERSAUT' : 'Contri_car_pol','PBESAUT' : 'Contri_deliv_van_pol',\n",
    "                   'PMOTSCO' : 'Contri_motorcycle/scooter_pol','PVRAAUT' : 'Contri_lorry_pol','PAANHANG' : 'Contri_trailer_pols',\n",
    "                   'PTRACTOR' : 'Contri_tractor_pol','PWERKT' : 'Contri_ag_machine_pol','PBROM' : 'Contri_moped_pol',\n",
    "                   'PLEVEN' : 'Contri_life_insur','PPERSONG' : 'Contri_prv_accid_insur_pol',\n",
    "                   'PGEZONG' : 'Contri_fam_accid_insur_pol','PWAOREG' : 'Contri_disabl_insur_pol','PBRAND' : 'Contri_fire_pol',\n",
    "                   'PZEILPL' : 'Contri_surfb_pol','PPLEZIER' : 'Contri_boat_pol','PFIETS' : 'Contri_bike_pol',\n",
    "                   'PINBOED' : 'Contri_prop_insur_pol','PBYSTAND' : 'Contri_ss_insur_polo','AWAPART' : 'Num_prv_3p_insur',\n",
    "                   'AWABEDR' : 'Num_firm_3p_insur','AWALAND' : 'Num_ag_3p_insur','APERSAUT' : 'Num_car_pol',\n",
    "                   'ABESAUT' : 'Num_deliv_van_pol','AMOTSCO' : 'Num_motorcycle/scooter_pol', 'AVRAAUT' : 'Num_lorry_pol','AAANHANG': 'Num_trailer_pol',\n",
    "                   'ATRACTOR' : 'Num_tractor_pol','AWERKT' : 'Num_ag_machines_pol','ABROM' : 'Num_moped_pol',\n",
    "                   'ALEVEN' : 'Num_life_insur_pol', 'APERSONG' : 'Num_prv_accid_insur_pol','AGEZONG' : 'Num_fam_ccid_insur_pol',\n",
    "                   'AWAOREG' : 'Num_disabl_insur_pol','ABRAND' :'Num_fire_pol','AZEILPL' :'Num_surfb_pol','APLEZIER' :'Num_boat_pol',\n",
    "                   'AFIETS' :'Num_bike_pol','AINBOED' :'Num_prop_insur_pol','ABYSTAND' :'num_ss_insur_pol', 'CARAVAN' : 'Target'},\n",
    "          inplace=True)\n",
    "\n",
    "\n",
    "test_df= pd.concat([eval1, target])\n",
    "\n",
    "print(df_td.describe())\n",
    "print(test_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SweetViz for visualizing the overall data to determine where to further investigate\n",
    "#You will need to have a full screen to see whats on the right side.\n",
    "# for more information you can visit the documentation here: https://pypi.org/project/sweetviz/\n",
    "import sweetviz as sv\n",
    "#config reports\n",
    "#Configuring the reports, early attempts automatically catagorized MOSTYPE and PWAPART as numberical rather than categorical\n",
    "cfg_1 = sv.FeatureConfig(force_cat=['subtype_L0'])\n",
    "\n",
    "report_train = sv.analyze([df_td, \"Train_data\"], target_feat = \"Target\", feat_cfg=cfg_1)\n",
    "report_train.show_html(\"Report_train.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Thoughts on Data Above:\n",
    "According to the Insurance challange the data is split into 2 sections: Demographics 0-42 and Policy ownership 43-86.The latter includes our target. Additionally, the features are based on the zipcodes which is unknown.\n",
    "    Note: you can hover over areas of the report or click the different tabs and that will pull up the bar charts and associations.\n",
    "\n",
    "\n",
    "#### 1) The target variable is heavily imbalanced.\n",
    "With only 348 instances of there being a Caravan policy; this will need to be addressed in order to prevent overfitting. I will undersample the instances which there is no Caravan policy and use ComplementNB to solve this issue.\n",
    "\n",
    "#### 2) There are 28 features in total that either provide information or receive information from the target variable and provide a good starting point for further investigation. Below are those features:\n",
    "Provides information on the target variable:\n",
    "'Contri_car_pol','Contri_fire_pol','Num_car_pol','subtype_L0','maintype_L2','Avg_inc','PP_cls','Inc_u_30','L_lvl_edu',\n",
    "'Contri_prv_3p_insur','N_car','Num_prv_3p_insur','R_house','O_house'\n",
    "\n",
    "Target variable provides information on the following:\n",
    "'Num_boat_pol','Contri_boat_pol','Contri_surfb_pol','Num_surfb_pol','num_ss_insur_pol','Contri_ss_insur_polo','Contri_car_pol','Num_disabl_insur_pol','Contri_disabl_insur_pol','Contri_fam_accid_insur_pol','Num_car_pol','Num_fam_ccid_insur_pol','Num_ag_machines_pol','Contri_fire_pol'\n",
    "\n",
    "    note: Sweetviz uses the uncertain coefficient to measure relationships of catagorical data.\n",
    "\n",
    "#### 4) It becomes immediately evident that there is some overlap on the information.\n",
    "On the associations page; we can also see there is quite a bit of noise that arises due to the direct relationships that features have with each other.\n",
    "\n",
    "For example, Contri_car_pol, Num_car_pol, and Contri_prv_3p_insur all are related in that those with insurance(Contri_prv_3p_insur), generally will contribute to some form of insurance, in this case car insurance. That being said, the contribution to car policies would be related to the number of car policies in the given zip code.\n",
    "\n",
    "Additionally, PP_cls has a direct relationship with the Subtype_L0 class; intuitively, we can remove one without loosing much informaiton on the other. \n",
    "\n",
    "Because of this, we can quickly deduce that many of the variables are probably not going to have any impact on our Target. So I will be selectiving certain variables that appear to have no relation.\n",
    "\n",
    "### But first, lets explore the features that were originally highlighted and add a few more features as they are directly related:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am going to start off by creating a new Dataframe made up of only the features that were mentioned above, along with features that directly relate.\n",
    "modified_df = df_td[['Contri_car_pol','Num_car_pol','Contri_fire_pol','Num_fire_pol','maintype_L2','Avg_inc','PP_cls','Inc_u_30k',\n",
    "                     'Inc_btw_30_45k','Inc_btw_45_75k','Inc_75_122k','Inc_ovr_123k',\n",
    "                     'L_lvl_edu', 'H_lvl_edu','M_lvl_edu','Contri_prv_3p_insur','Num_prv_3p_insur','N_car', '1_car','2_cars',\n",
    "                     'R_house','O_house', 'Num_boat_pol','Contri_boat_pol','Contri_surfb_pol','Num_surfb_pol','num_ss_insur_pol',\n",
    "                     'Contri_ss_insur_polo','Num_disabl_insur_pol','Contri_disabl_insur_pol','Contri_fam_accid_insur_pol',\n",
    "                     'Num_fam_ccid_insur_pol','Num_ag_machines_pol','Target']]\n",
    "\n",
    "modified_df_report = sv.analyze([modified_df, \"Combined\"], target_feat = \"Target\")\n",
    "modified_df_report.show_html(\"Mod_Report_Combined.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at this report, we can get a good sense as to which subfeatures to further explore after using LassoCV to find which features I'll be using.\n",
    "\n",
    "### Below I am implementing the LassoCV method for feature extraction on the modified_df in order to find which features can be kept and which will be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using LassoCV to select features and see what it identifies from the post encoded dataset and seeing if it matches up with my earlier findings.\n",
    "# I am using the LassoCV method for its ability to easily retain the original features since my question is who is most likely to purchase a policy.\n",
    "# Thus giving a better understanding for the relationships amoung variables.\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "X = modified_df.drop('Target', axis=1)\n",
    "y = modified_df.Target\n",
    "reg = LassoCV(max_iter=100000, tol=0.00001)\n",
    "reg.fit(X, y)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" % reg.score(X, y))\n",
    "coef = pd.Series(reg.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a chart to make ease of understanding the strength between relationships.\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0))+ \" variables\")\n",
    "\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (10, 100)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_labels = imp_coef[imp_coef != 0]\n",
    "kept_labels.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the results above, we some interesting occurances that will add to how features are chosen:\n",
    "1) The Contribution and Number of surfboard policies was eliminated by the LassoCv completely, whereas the Number of car policies was dropped and the Contribution to car policies was kept.\n",
    "This is an interesting discovery because the contribution and number of policies are directly related. The contribution factor measures what level of premium the policy holder pays, (higher the number the higher the policy); and the number of policies is how many are taken out.\n",
    "\n",
    "Because there are similar occurances with other contribution/number policies such as with Social Security (ss), it is safe to assume that should one be retained, the dropped label probably has a subsect that has relevance and can be retained.\n",
    "\n",
    "2) The 45k-75k income level was dropped, while the other 3 were retained.\n",
    "This is interesting because generally one would assume with greater income, more insurance would be taken out. However, with that particular range removed this could either be signalling that income level is irrelavent and another factor has more statistical importance or individuals in that income range just like to live life on the edge.\n",
    "\n",
    "#### I am leaning towards the prior for 2 reasons: \n",
    "   1) The higher income variable (Inc_ovr_123k) has a negative coefficient; additionally  \n",
    "   2) Purchasing Power class (PP_cls) was retained and is intuitively related to income level. From what was observed earlier, the PP_cls level 6-       8 had the strongest predictive value.\n",
    "   \n",
    "## Because of these findings, I believe it is appropriate to further break down dataframe into a dummy table to view which variables really impact our target.\n",
    "\n",
    "I will be dropping the income features, surfboard policies,and keep purchasing power eliminate the effect redundant effect from income level and purchasing power and remove an eliminated feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_df = df_td[['Num_ag_machines_pol', 'R_house', 'O_house', 'Num_fire_pol',\n",
    "                'L_lvl_edu', 'Num_prv_3p_insur', 'M_lvl_edu',\n",
    "                'Num_car_pol', 'H_lvl_edu',\n",
    "                'PP_cls', 'N_car', '2_cars', 'num_ss_insur_pol',\n",
    "                'Avg_inc', '1_car', 'Contri_boat_pol', 'Contri_fire_pol',\n",
    "                'Contri_car_pol', 'Contri_prv_3p_insur', 'Contri_disabl_insur_pol',\n",
    "                'Contri_ss_insur_polo', 'Contri_fam_accid_insur_pol', 'Num_boat_pol','Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dummy tables and leaving our Target variable out for the models.\n",
    "pre_enc= mod_df.drop('Target', axis=1)\n",
    "post_enc_df = pd.get_dummies(mod_df, prefix_sep=\"_\", columns=pre_enc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using LassoCV to select features and see what it identifies from the post encoded dataset and seeing if it matches up with my earlier findings.\n",
    "# I am using the lassoCV method for its ability to easily retain the original features since my question is who is most likely to purchase a policy.\n",
    "# Thus giving a better understanding for the relationships amoung variables.\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "X = post_enc_df.drop('Target', axis=1)\n",
    "y = post_enc_df.Target\n",
    "reg = LassoCV(max_iter=100000, tol=0.00001)\n",
    "reg.fit(X, y)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" % reg.score(X, y))\n",
    "coef = pd.Series(reg.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a chart to make ease of understanding the strength between relationships.\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0))+ \" variables\")\n",
    "\n",
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (10, 100)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kept_labels = imp_coef[imp_coef != 0]\n",
    "kept_labels.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The lasso above reveals some more interesting results:\n",
    "We see that the most important features currently are the Contributions to car policies at 60%, contribution to fire policies at 40%, and Purchasing power class level 7. Additionally, we can see that having a car policy seems to be an important factor as well. Because of this, dropping the N_car feature will have some impact on our results.\n",
    "\n",
    "Keeping these features in mind, we will create the new dataframe to be used for the baseline model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using LassoCV, we now have our new features\n",
    "new_df = post_enc_df[['Contri_boat_pol_0', 'Contri_fire_pol_2', 'H_lvl_edu_0', 'O_house_7',\n",
    "                      'H_lvl_edu_2', 'H_lvl_edu_3', 'Avg_inc_3', 'L_lvl_edu_6',\n",
    "                      'Contri_fire_pol_6', 'M_lvl_edu_0', 'PP_cls_3', '1_car_4', 'R_house_4',\n",
    "                      'L_lvl_edu_7', 'L_lvl_edu_9', 'Num_car_pol_1',\n",
    "                      'Num_boat_pol_0', 'O_house_9', '2_cars_1',\n",
    "                      '1_car_5', 'L_lvl_edu_2', 'R_house_0', 'Contri_prv_3p_insur_2',\n",
    "                      'L_lvl_edu_1', 'Avg_inc_4', 'Avg_inc_5', '1_car_7', 'Avg_inc_7',\n",
    "                      'Contri_fire_pol_3', 'Num_car_pol_2', 'num_ss_insur_pol_1', 'PP_cls_7',\n",
    "                      'Contri_fire_pol_4', 'Contri_car_pol_6','Target']]\n",
    "\n",
    "new_report = sv.analyze([new_df, \"Combined\"], target_feat = \"Target\")\n",
    "new_report.show_html(\"New_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now its time to develop a baseline model:\n",
    "For this problem I will be using Native Bayes, Ensemble methods, decision trees, and a neural net to see what works best with the data. I still have to address the imbalance, but I want to see how these models handle the data with the selected features.\n",
    "\n",
    "I have decided to use these particular models as this is a classification and prediction problem. First identifying which \n",
    "\n",
    "One of the more interesting models I will be using is the ComplementNB(), the primary purpose of it is applying native bayes specifically to compensate for imbalanced data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, plot_roc_curve, classification_report, balanced_accuracy_score, coverage_error\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import ComplementNB, CategoricalNB, BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def clf_comp(df):\n",
    "    classifiers = [\n",
    "    ComplementNB(),\n",
    "    CategoricalNB(),\n",
    "    BernoulliNB(),\n",
    "    MultinomialNB(),\n",
    "    GaussianNB(),\n",
    "    MLPClassifier(hidden_layer_sizes=(10,10,10), max_iter=1000),\n",
    "    RandomForestClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Separating out the features\n",
    "    X =  df.drop('Target', axis=1) #df_td.drop('Target', axis=1)# train_feat\n",
    "    # Separating out the target\n",
    "    y = df.Target #df_td.Target#target_feat\n",
    "    \n",
    "   \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "    for classifier in classifiers:\n",
    "        scores = cross_val_score(classifier, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "        model = classifier.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        print(classifier)\n",
    "        print('The Training F1 Score is', f1_score(classifier.predict(X_train), y_train))\n",
    "        print('The Testing F1 Score is', f1_score(predictions, y_test))\n",
    "        print(\"accuracy score\" '\\n', accuracy_score(y_test, predictions))\n",
    "        print(\"balanced_accuracy_score\" '\\n', balanced_accuracy_score(y_test, predictions))\n",
    "        print(\"model confusion matrix\" '\\n', confusion_matrix(y_test, predictions, normalize=\"all\"))\n",
    "        print(\"classification_report\" '\\n', classification_report(y_test, predictions),'\\n')\n",
    "        ax = plt.gca()\n",
    "        plt.rcParams['figure.figsize'] = (10, 10)\n",
    "        disp = plot_roc_curve(classifier, X_test,y_test, ax=ax, alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_comp(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a baseline for the model, we can see what works.\n",
    "\n",
    "Without any hyper parameter tuning, the native bayes classifiers stand out compared to the ensemble and neural net methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "#Creating a 5-1 ratio to test how this effects that data.\n",
    "# separate minority and majority classes\n",
    "no_policy = new_df[post_enc_df.Target==0][:1740]\n",
    "has_policy = new_df[post_enc_df.Target==1]\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([no_policy, has_policy])\n",
    "\n",
    "f_df = upsampled.sample(frac=1)\n",
    "\n",
    "# check new class counts\n",
    "print(f_df.Target.value_counts())\n",
    "print(f_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_comp(f_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
